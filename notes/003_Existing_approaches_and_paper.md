## Existing approaches

### 1. Microsoft Distributed Machine Learning Toolkit (DMTK)
Open source framework for distributing model training across multiple nodes. Supports data parallelization, pipelining, model scheduling, distributed word embeddings (incl. Word2Vec) and much more.
[Link](http://www.dmtk.io/)

### 2. DeepLearning4j (DL4J)
Open source, distributed deep learning library for the JVM. Build on top of Hadoop and Spark. Also implements a solution for Word2Vec.
[Link](https://deeplearning4j.org/)

## Existing papers

### 1. Large Scale Distributed Deep Networks (DistBelief)

[Link](http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf)
[DistBelief in Akka](http://alexminnaar.com/2015/09/06/DistBelief-with-Akka.html)

### 2. SPARKNET: TRAINING DEEP NETWORKS IN SPARK

[Link](https://arxiv.org/pdf/1511.06051.pdf)

### 3. Parallelizing Word2Vec in Shared and Distributed Memory

[Link](https://arxiv.org/abs/1604.04661)

### 4. Network-Efficient Distributed Word2vec Training System for Large Vocabularies

[Link](https://arxiv.org/abs/1606.08495)

### 5. Neuronale Netze

[Link](http://www.neuronalesnetz.de/downloads/neuronalesnetz_de.pdf)

### 6. Distributed Neuronale Networks

[Link](https://blog.skymind.ai/distributed-deep-learning-part-1-an-introduction-to-distributed-training-of-neural-networks/)
