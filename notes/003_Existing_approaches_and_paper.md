## Existing approaches

### 1. Microsoft Distributed Machine Learning Toolkit (DMTK)
Open source framework for distributing model training across multiple nodes. Supports data parallelization, pipelining, model scheduling, distributed word embeddings (incl. Word2Vec) and much more.
[Link](http://www.dmtk.io/)

### 2. DeepLearning4j (DL4J)
Open source, distributed deep learning library for the JVM. Build on top of Hadoop and Spark. Also implements a solution for Word2Vec.
[Link](https://deeplearning4j.org/)

## Existing papers

### 1. Large Scale Distributed Deep Networks (DistBelief)

* [Link](http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf)
* [DistBelief in Akka](http://alexminnaar.com/2015/09/06/DistBelief-with-Akka.html)

### 2. SPARKNET: TRAINING DEEP NETWORKS IN SPARK

[Link](https://arxiv.org/pdf/1511.06051.pdf)

### 3. Parallelizing Word2Vec in Shared and Distributed Memory

[Link](https://arxiv.org/abs/1604.04661)

### 4. Network-Efficient Distributed Word2vec Training System for Large Vocabularies

[Link](https://arxiv.org/abs/1606.08495)

### 5. Asynchronous Training of Word Embeddings for Large Text Corpora

[Link](https://arxiv.org/pdf/1812.03825.pdf)

### 6. Neuronale Netze

[Link](http://www.neuronalesnetz.de/downloads/neuronalesnetz_de.pdf)

### 7. Distributed Neuronale Networks

[Link](https://blog.skymind.ai/distributed-deep-learning-part-1-an-introduction-to-distributed-training-of-neural-networks/)

### 8. Neural Networks in Akka

* [Artificial Neural Networks in Akka](https://www.academia.edu/29815486/Artificial_Neural_Networks_in_Akka)
* [Decentralized and Distributed Machine Learning Model Training with Actors](http://www.scs.stanford.edu/17au-cs244b/labs/projects/addair.pdf)

### 9. Distributed Negative Sampling for Word Embeddings

[Link](https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewFile/14956/14446)

### 10. Word2Vec Tutorial

[Link](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)
